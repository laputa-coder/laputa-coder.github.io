---
layout: post
title: "卷积神经网络"
date: 2021-09-19 18:18
categories: [Machine Learning]
tags: [神经网络, CNN, MachineLearning]
---

# 概述
当前比较流行的神经网络是深度神经网络（deep convolutional neural networks,CNNs)

> CNNs 可以自动从（通常是大规模）数据中学习特征，并把结果向同类型未知数据泛化。

感知机是神经网络和支持向量机的基础。20世纪60年代，当时的数学证明表明，如果数据线性可分，感知机可以在有限的次数内收敛。感知机的解是超平面参数集。

感知机的瓶颈：
- 感知机暂时没有有效的训练方法，导致层数无法加深
- 由于采用线性激活函数，导致无法处理线性不可分问题

随着后向传播（BP）算法和非线性激活函数的提出得到解决。

机器进行模式识别的四个阶段：
- 数据获取
- 预处理
- 特征提取
- 数据分类

> CNN 目前是图像领域特征提取最好的方式。

# 网络结构

基础的CNN由 卷积(convolution), 激活(activation), and 池化(pooling)三种结构组成。CNN输出的结果是每幅图像的特定特征空间。当处理图像分类任务时，我们会把CNN输出的特征空间作为全连接层或全连接神经网络(fully connected neural network, FCN)的输入，用全连接层来完成从输入图像到标签集的映射，即分类。当然，整个过程最重要的工作就是如何通过训练数据迭代调整网络权重，也就是后向传播算法。目前主流的卷积神经网络(CNNs)，比如VGG, ResNet都是由简单的CNN调整，组合而来。

## CNN

![](/assets//assets/images/img_HandsOnML/img_ML/某个stage内CNN工作原理.jpg)

如上图，显示的是CNN的基础结构，现在的大型卷积神经网络（CNNs)通常由多个上述结构前后连接，层内调整得到的，根据功能不同，我们称这些前后连接的结构处于不同阶段(stage)。虽然在主流CNNs中，不同stage里CNN会有不同的单元和结构，比如卷积核 (kernel)大小可能不同，激活函数(activition function) 可能不同，pooling操作可能不存在，但是图1的CNN结构应当能够包含所有的情况。

如上图，一个stage中的一个CNN，通常会由三种映射空间组成

- 输入映射空间(input maps volume）
- 特征映射空间(feature maps volume）
- 池化映射空间(pooled maps volume)

例如图中，输入的是彩色RGB图像，那么输入的maps volume由红，黄，蓝三通道/三种map构成。我们之所以用input map volume这个词来形容，是因为对于多通道图像输入图像实际上是由高度，宽度，深度三种信息构成,可以被形象理解为一种"体积"。这里的“深度”，在RGB中就是3，红，黄，蓝三种颜色构成的图像，在灰度图像中，就是1。

## 卷积

CNN中最基础的操作是卷积convolution，再精确一点，基础CNN所用的卷积是一种2-D卷积。也就是说，kernel只能在x,y上滑动位移，不能进行深度 (跨通道) 位移。这可以根据图1来理解，对于图中的RGB图像，采用了三个独立的2-D kernel，如黄色部分所示，所以这个kernel的维度是

$$
X * Y * 3
$$

在基础CNN的不同stage中，kernel的深度都应当一致，等于输入图像的通道数。
卷积需要输入两个参数，实质是二维空间滤波，滤波的性质与kernel选择有关，CNN的卷积是在一个2-D kernel 和输入的 2-D input map 之间，RGB中各图像通道分别完成。

我们假设单一通道输入图像的空间坐标为(x,y) ,卷积核大小是 p*q ,kernel权重为 w ,图像的亮度值是 v .

卷积过程就是**kernel所有权重与其在输入图像上对应元素亮度之和**，可以表示为

$$
conv_{x,y}=\sum_i^{p*q} w_i v_i
$$

举例：
![](/assets//assets/images/img_HandsOnML/img_ML/卷积计算.jpg)

如上图所示，这时候卷积计算结果为：

$$
conv_{x,y}=105*0+102*(-1)+100*0+.....+98*(-1)+104*0=89
$$

并将kernel随(x,y)平移扫描，可以得到输出空间，这时假设输入图像大小是 512*512 。卷积核是 3 *3 在不考虑零填充的情况下，输出是
$$
512-3+1= 510*510
$$
注意卷积层的kernel可能不止一个，扫描步长，方向也有不同，这些进阶方式可以归纳一下:
- 可以采用多个卷积核，设为n 同时扫描，得到的feature map会增加n个维度，通常认为是多抓取n个特征。
- 可以采取不同扫描步长，比如上例子中采用步长为n, 输出是 $ (510/n,510/n) $
- padding，上例里，卷积过后图像维度是缩减的，可以在图像周围填充0来保证feature map(特征映射空间or输出的图片）与原始图像大小不变

- 深度升降，例如采用增加一个1*1 kernel来增加深度，相当于复制一层当前通道作为feature map

- 跨层传递feature map,不再局限于输入即输出, 例如ResNet跨层传递特征，Faster RCNN 的POI pooling.

## 激活

卷积之后，通常会加入偏置(bias), 并引入非线性激活函数(activation function)，这里定义bias为b，activation function 是 h(x),经过激活函数之后，得到的结果是
$$
z_{x,y} =h(\sum _i^{p*q}w_iv_i+b)
$$

> 这里请注意，bias不与元素位置相关，只与层有关。

主流的activation function 有：

- 线性整流单元（RelU）
$$
h(z)=max(0,z)
$$
![](/assets//assets/images/img_HandsOnML/img_ML/线性整流单元.jpg)

- Sigmoid函数
$$
h(z)=1/(1+e^{-z})
$$
![](/assets//assets/images/img_HandsOnML/img_ML/Sigmoid函数.jpg)
- tanh函数
$$
h(z)=tanh(z)
$$
![](/assets//assets/images/img_HandsOnML/img_ML/tanh函数.jpg)

图1中的 feature maps valume 的每个元素是由 $z_{z,y}$ 组成的。

我们可以回到图1的上半部分，这里的feature map是可以可视化的。

![](/assets//assets/images/img_HandsOnML/img_ML/某个stage内CNN工作原理.jpg)

例如采用277* 277的RGB图像， 采用96个11* 11* 3的kernels同时扫描，很容易得到输出的feature maps是96个267* 267的二维 feature map, 267*267是单个图像feature map的x,y轴大小，96是卷积核个数，原本的3通道在积分的时候会被作为一个元素加起来。 如上图，这些feature map可视化之后，可以看到4 和35表示边缘特征，23是模糊化的输入，10和16在强调灰度变化，39强调眼睛，45强调红色通道的表现。

## 池化
池化(pooling），是一种降采样操作(subsampling)，主要目标是降低feature maps的特征空间，或者可以认为是降低feature maps的分辨率。因为feature map参数太多，而图像细节不利于高层特征的抽取。

![](/assets//assets/images/img_HandsOnML/img_ML/池化.jpg)

目前主要的池化（pooling）操作有：
- 最大值池化 Max pooling：如上图所示，2 * 2的max pooling就是取4个像素点中最大值保留
- 平均值池化 Average pooling: 如上图所示, 2 * 2的average pooling就是取4个像素点中平均值值保留
- l2池化 l2 pooling: 即取均方值保留

Pooling操作会降低参数，降低feature maps的分辨率，但是这种暴力降低在计算力足够的情况下是不是必须的，并不确定。目前一些大的CNNs网络只是偶尔使用pooling.

以上是一个CNN stage的基本结构，需要强调的是，这个结构是可变的，目前大部分网络都是根据基本结构堆叠调整参数，或跳层连接而成。CNN的输出是feature maps，它不仅仅可以被输入全连接网络来分类，也可以接入另外一个“镜像”的CNN，如果输入图像维度与这个新的CNN输出feature maps特征维度相同，即这个新接入的CNN在做上采样, upsampling， 得到的图像可以认为是在做像素级的标注，图像分割

# 全连接网络
出现在CNN中的全连接网络(fully connected network)主要目的是为了分类, 这里称它为network的原因是，目前CNNs多数会采用多层全连接层，这样的结构可以被认为是网络。如果只有一层，下边的叙述同样适用。它的结构可能如下图所示:

![](/assets//assets/images/img_HandsOnML/img_ML/全连接网络.jpg)

不同于CNN的滑动卷积，全连接网络每一层的所有单元与上一层完全连接。通常，除了输入层和输出层的其他层，都被认为是隐含层。

如图所示，对于第l层的第i个神经元，它的输出计算方式是
$$
z_i(l)=\sum_{j=i}^{n_{l-1}}w_{ij}(l)a_j(l-1)+b_i(l)
$$
考虑activation function之后，对于第l 层的第i个神经元，输出是
$$
a_i(l)=h(z_i(l))
$$

计算这一层中的所有神经元之后, 作为下一层的输入。

全连接网络和CNN的数学表达结构其实很相似，只是不存在关于图像空间上的滑动卷积。

# 目标函数与训练方法
CNN网络的训练误差需要通过一个目标函数来衡量，目前比较流行的目标函数是均方误差(Mean Square Error)和K-L散度（K-L divergence)，对于输出层的误差公式很容易判断:

- MSE：

$$
E={1\over 2}{\sum _{j=1} ^{n_l}(r_j-a_j(l))^2}
$$

- K-L divergence:

$$
E=-{1\over {n_l}} \sum_{j=1} ^{n_l} [r_j ln a_j(l)+(1-r)ln(1-a_j(l))]
$$

参数说明：
- $r_j$ :是期望输出（标注标签）
- $a_j(l)$ :是第l层的第j个神经元的输出

> 通常情况下，K-ldivergence的权重更新会比MSE更快，不过本文将通过MSE来举例说明。

如果我们仅仅考虑最后一层的更新，通过梯度下降，权重$w_{i,j}$和$b_i$的更新方式：

将 $a_j(l)$带入公式求导就可以算出
$$
w_{ij}(l)=w_{ij}(l)-\alpha { {\nabla E}\over{\nabla w_{ij}(l)} }
$$
和
$$
b_i(l)=b_i(l)-\alpha{ {\nabla E}\over{\nabla b_i(l)} }

$$

其中：
- $\alpha$ :是learning rate, 如果learning rate 取值过大，可能会收敛于震荡，如果learning rate取值过小，可能收敛速度太慢。

    > 以上是如果网络只有最后一层的训练方式，但是实际上对于深层网络，我们很难一次通过数学计算出每一层的权重更新公式，也就是权重很难更新。

    可以看出，如果想要训练网络，就需要根据误差更新权重，而如果想要获得误差

- E: 目标函数。 不论是MSE,还是K-l divergence, 都需要两种参数
    - $r_j$ : 期望输出（标注标签）
    - $a_j(l)(第l层的第j个神经元的输出)==z_i(l)(回顾公式）$ ：即需要参数 w,b
    其中，r 来自于标签，很容易获得，而$a_j(l)$和误差E 相互影响。

那么，解决方式就很明显，我们可以先固定一方，更新另一方，这是alternating optimazition优化多参数模型的经典思路。CNN的训练方法思路也来自于此，被称作（反向传播）。
# 反向传播算法

![](/assets//assets/images/img_HandsOnML/img_ML/反向传播.jpg)

Backpropagation算法,大概分为两步：

1. 通过训练数据计算网络中的所有$a_j(l)$ 
    - $a_j(l)$ 的计算方法，最初的$a_j(l)$ 只需要输入图像和初始权重就可以计算，这一步是从输入图像到输出层的计算，即上图中的前向传播。

2. 获得所有的$a_j(l)$之后，我们就可以通过目标函数和期望输出计算出最后一层的E，而有了最后一层的E，可以计算出倒数第二层的期望输出$a_j(l)$

3. 以此类推，可以计算误差到第一层，并通过求导更新权重。这是上图的后向误差传播（这里表述不严谨)。
上述1，2部操作会交替进行。


实际上BP算法通过以下四个公式更新:

公式一
$$
{ {\nabla E}\over{\nabla w_{ij}(l)}}=a_j(l-1)\Delta _i(l) 
$$
公式二
$$
{ {\nabla E}\over{\nabla b_{i}(l)}}=\Delta _i(l)
$$
公式三
$$
\Delta _j(l)=h ^{'}(z_j(l))\sum _i w_{ij}(l+1)\Delta _i (l+1)
$$
公式四
$$
\Delta _j(l)=h^{'}(z_j(l))[a_j(l)-r_j]
$$


说明：
公式一和公式二 用来计算更新权重 $w_{i,j}$ 和 $b_i$(bias) 所需的梯度。

公式三和公式四 是公式二和公式三中未知项的来源，公式四用来计算最后一层梯度，公式三用来计算除最后一层外其他层的梯度,并通过传播梯度来传递误差，其中activition function的梯度$h^{'}(z_j(l)), l=1,.....L $和各层权重$a_i(l) ,l=1,...L$ 都可以在前向传播过程中计算出来。


严谨的BP算法流程:
1. 用随机小数初始化所有权重$w_{i,j}$ 和 $b_i$

2. 利用来自训练集的输入向量(例如一副图像)，算出所有的 $a_j(l)$和$h^{'}(z_j(l))

3. 用公式 计算 MSE或K-L divergence

4. 用公式四计算 $\Delta _j(L)$, 并后向传播，用(4)计算出所有其他层的 $\Delta _j(l) ,l=L-1,L-2 ...2$

5. 更新权重

6. 对训练集中的所有输入向量(图像)重复 2-5，完成一次所有训练成为一个epoch。当MSE误差稳定不变，或者到达某个迭代次数后，BP算法停止。
